{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLI1tKRiZYN0xf/DC3TxtV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Machine%20Learning%20Interview%20Prep%20Questions/Reinforcement%20Learning%20Algorithms/Q-Learning/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning from Scratch\n",
        "\n",
        "This notebook demonstrates how to implement the Q-Learning algorithm **from scratch** without using any external reinforcement learning libraries. It uses a simple 4x4 grid world to teach the core concepts behind Q-Learning.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Q-Learning?\n",
        "\n",
        "Q-Learning is a **model-free, off-policy, value-based** reinforcement learning algorithm. It helps an agent learn **optimal actions** in an environment by maximizing long-term rewards.\n",
        "\n",
        "### Q-Learning Update Rule (Bellman Equation)\n",
        "\n",
        "$$\n",
        "Q(s, a) ← Q(s, a) + α * [r + γ * max(Q(s', a')) − Q(s, a)]\n",
        "$$\n",
        "\n",
        "\n",
        "```\n",
        "- `s`: current state\n",
        "- `a`: action taken\n",
        "- `r`: reward received\n",
        "- `s'`: next state\n",
        "- `α`: learning rate\n",
        "- `γ`: discount factor\n",
        "```\n",
        "\n",
        "\n",
        "## Define Environment\n",
        "We'll use a 4x4 Grid. Start at top-left `(0,0)` and goal is bottom-right `(3,3)`.\n",
        "\n"
      ],
      "metadata": {
        "id": "JVieOGIM0K2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid size\n",
        "n_rows, n_cols = 4, 4\n",
        "state_space = n_rows * n_cols\n",
        "action_space = 4  # up, down, left, right\n",
        "\n",
        "# Define actions\n",
        "actions = {\n",
        "    0: (-1, 0),  # up\n",
        "    1: (1, 0),   # down\n",
        "    2: (0, -1),  # left\n",
        "    3: (0, 1)    # right\n",
        "}\n",
        "\n",
        "# Goal state position\n",
        "goal_state = (3, 3)\n",
        "\n",
        "# Convert (row, col) to state index\n",
        "def pos_to_state(row, col):\n",
        "    return row * n_cols + col\n",
        "\n",
        "# Convert state index to (row, col)\n",
        "def state_to_pos(state):\n",
        "    return divmod(state, n_cols)\n"
      ],
      "metadata": {
        "id": "GJFbd4WU04Lc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Environment Dynamics\n",
        "We'll simulate movement, apply bounds, and give reward.\n"
      ],
      "metadata": {
        "id": "niqNTwQe07xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step(state, action):\n",
        "    row, col = state_to_pos(state)\n",
        "    dr, dc = actions[action]\n",
        "\n",
        "    # Move\n",
        "    new_row = min(max(row + dr, 0), n_rows - 1)\n",
        "    new_col = min(max(col + dc, 0), n_cols - 1)\n",
        "\n",
        "    new_state = pos_to_state(new_row, new_col)\n",
        "\n",
        "    # Reward function\n",
        "    reward = 1 if (new_row, new_col) == goal_state else -0.01\n",
        "    done = (new_row, new_col) == goal_state\n",
        "\n",
        "    return new_state, reward, done"
      ],
      "metadata": {
        "id": "EWin80ri09Rw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Q-Table\n",
        "Rows = states, Columns = actions (Up, Down, Left, Right)\n"
      ],
      "metadata": {
        "id": "2B12W5z71BAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((state_space, action_space))"
      ],
      "metadata": {
        "id": "RWd7JqiG1CpZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters for Training\n"
      ],
      "metadata": {
        "id": "qNRkJeA81G6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1          # learning rate\n",
        "gamma = 0.99         # discount factor\n",
        "epsilon = 1.0        # exploration probability\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.01\n",
        "episodes = 500"
      ],
      "metadata": {
        "id": "vHxF4LCj1IiA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning Training Loop\n",
        "Agent explores and learns the optimal policy.\n"
      ],
      "metadata": {
        "id": "CBTdsYt51MGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(episodes):\n",
        "    state = pos_to_state(0, 0)  # Start from top-left\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # ε-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(action_space)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # Take step\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # Q-value update\n",
        "        td_target = reward + gamma * np.max(q_table[next_state])\n",
        "        q_table[state, action] += alpha * (td_target - q_table[state, action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Decay exploration\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)"
      ],
      "metadata": {
        "id": "nctG1fnl1NUs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Q-Table\n",
        "Shows learned values for each state-action pair.\n"
      ],
      "metadata": {
        "id": "Wm5sochu1P1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=2)\n",
        "q_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DGog-0-1SBS",
        "outputId": "03811c5a-8b6c-44f9-a74d-c3ff9c5b36b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.87, 0.85, 0.87, 0.9 ],\n",
              "       [0.85, 0.92, 0.86, 0.89],\n",
              "       [0.67, 0.94, 0.37, 0.31],\n",
              "       [0.16, 0.62, 0.1 , 0.22],\n",
              "       [0.36, 0.92, 0.46, 0.54],\n",
              "       [0.87, 0.94, 0.81, 0.91],\n",
              "       [0.63, 0.96, 0.75, 0.72],\n",
              "       [0.26, 0.92, 0.55, 0.42],\n",
              "       [0.49, 0.62, 0.63, 0.94],\n",
              "       [0.87, 0.94, 0.91, 0.96],\n",
              "       [0.93, 0.98, 0.94, 0.96],\n",
              "       [0.51, 1.  , 0.6 , 0.73],\n",
              "       [0.38, 0.5 , 0.43, 0.89],\n",
              "       [0.65, 0.73, 0.59, 0.98],\n",
              "       [0.95, 0.98, 0.94, 1.  ],\n",
              "       [0.  , 0.  , 0.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derive Optimal Policy\n",
        "Use arrows to visualize which action the agent prefers in each state.\n"
      ],
      "metadata": {
        "id": "jmvh1IE71V92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = np.array([np.argmax(q_table[s]) for s in range(state_space)])\n",
        "policy = policy.reshape((n_rows, n_cols))\n",
        "\n",
        "# Map to arrows\n",
        "arrow_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "for row in policy:\n",
        "    print(' '.join([arrow_map[a] for a in row]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAdm0vHO1XId",
        "outputId": "29e81f14-f3c4-425d-9c2e-5cc227fd1b86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ ↓ ↓ ↓\n",
            "↓ ↓ ↓ ↓\n",
            "→ → ↓ ↓\n",
            "→ → → ↑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- Implemented Q-learning **without classes**\n",
        "- Built a custom grid environment from scratch\n",
        "- Visualized the learned policy\n",
        "- Covered ε-greedy, reward shaping, and Bellman updates\n",
        "\n",
        "Use this as a foundational notebook for understanding **how agents learn** through trial, error, and reward-based learning!\n"
      ],
      "metadata": {
        "id": "mIoFUuZ91b8w"
      }
    }
  ]
}