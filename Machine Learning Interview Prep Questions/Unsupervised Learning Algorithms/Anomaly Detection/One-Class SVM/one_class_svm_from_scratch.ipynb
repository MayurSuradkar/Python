{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy936N5ttVFqa+IRQtxxjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Machine%20Learning%20Interview%20Prep%20Questions/Unsupervised%20Learning%20Algorithms/Anomaly%20Detection/One-Class%20SVM/one_class_svm_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is One-Class SVM?\n",
        "**One-Class SVM** is an unsupervised **anomaly detection algorithm** that learns the boundary of normal data and identifies whether new observations fall **inside (normal)** or **outside (anomaly)** this boundary.\n",
        "\n",
        "It’s based on Support Vector Machines and works well when you only have normal data during training.\n",
        "\n",
        "## Goal\n",
        "\n",
        "Find a function `f(x)` such that:\n",
        "\n",
        "* `f(x) ≥ 0` for most normal data points\n",
        "* `f(x) < 0` for outliers (anomalies)\n",
        "\n",
        "We will use a **Gaussian (RBF) kernel** and solve a simplified dual form of the One-Class SVM problem.\n",
        "\n",
        "\n",
        "## Assumptions for Simplicity\n",
        "* Only radial basis function (RBF) kernel is used.\n",
        "* No kernel matrix optimization (done via loops).\n",
        "* Dual form is simplified and optimized with gradient descent.\n",
        "* Output is purely educational (not production-speed).\n",
        "\n",
        "\n",
        "## Step-by-Step Implementation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dsdd19XcxYkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zl9D-Jn3yVhu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generate Normal Training Data (Only Normal Data)"
      ],
      "metadata": {
        "id": "WxLn1HGKyZdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Generate normal data (centered around origin)\n",
        "X_train = np.random.normal(0, 1, size=(100, 2))\n",
        "\n",
        "# Test data: normal + outliers\n",
        "X_test_normal = np.random.normal(0, 1, size=(10, 2))\n",
        "X_test_outliers = np.random.uniform(low=4, high=6, size=(5, 2))\n",
        "X_test = np.vstack((X_test_normal, X_test_outliers))"
      ],
      "metadata": {
        "id": "BE_tSjLTyaRo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define RBF (Gaussian) Kernel"
      ],
      "metadata": {
        "id": "zDjqAv3HydAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rbf_kernel(x1, x2, gamma=0.5):\n",
        "    \"\"\"Compute RBF kernel between two vectors.\"\"\"\n",
        "    distance_squared = np.linalg.norm(x1 - x2) ** 2\n",
        "    return np.exp(-gamma * distance_squared)"
      ],
      "metadata": {
        "id": "rOi_zYXtyhRv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Compute Full Kernel Matrix"
      ],
      "metadata": {
        "id": "aTcu3eiKyjn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_kernel_matrix(X, gamma=0.5):\n",
        "    n_samples = X.shape[0]\n",
        "    K = np.zeros((n_samples, n_samples))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_samples):\n",
        "            K[i, j] = rbf_kernel(X[i], X[j], gamma)\n",
        "    return K"
      ],
      "metadata": {
        "id": "FMo0JuG4ynTr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train One-Class SVM (Dual Problem)\n",
        "We optimize the dual objective:\n",
        "\n",
        "We optimize the **dual form** of the One-Class SVM:\n",
        "\n",
        "$$[\n",
        "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i K(x_i, x_i) - \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j K(x_i, x_j)\n",
        "]$$\n",
        "\n",
        "Subject to the constraints:\n",
        "\n",
        "$$[\n",
        "0 \\leq \\alpha_i \\leq \\frac{1}{\\nu n}, \\quad \\sum_{i=1}^{n} \\alpha_i = 1\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "- $$( \\alpha_i )$$ are the Lagrange multipliers (weights for each support vector)\n",
        "- $$( K(x_i, x_j) )$$ is the **RBF kernel** (or any other kernel)\n",
        "- $$( \\nu \\in (0, 1] )$$ controls the fraction of outliers\n",
        "- $$( n )$$ is the number of training samples\n",
        "\n",
        "\n",
        "In this notebook, we use **gradient descent** to optimize the dual objective:\n",
        "\n",
        "- Initialize $$( \\alpha_i = \\frac{1}{n} )$$\n",
        "- Update using the gradient:\n",
        "  $$[\n",
        "  \\alpha_i \\leftarrow \\alpha_i - \\eta \\cdot \\left( \\frac{\\partial L}{\\partial \\alpha_i} \\right)\n",
        "  ]$$\n",
        "- Project $$( \\alpha )$$ back to the feasible region:\n",
        "  - Clip to range $$( [0, \\frac{1}{\\nu n}] )$$\n",
        "  - Normalize to ensure $$( \\sum \\alpha_i = 1 )$$ using **simplex projection**\n",
        "\n",
        "This gives us a working set of $$( \\alpha )$$ weights that can be used for scoring test points.\n",
        "\n"
      ],
      "metadata": {
        "id": "uBDVISkIypdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_class_svm(X, nu=0.1, gamma=0.5, lr=0.01, n_iters=100):\n",
        "    n_samples = X.shape[0]\n",
        "    K = compute_kernel_matrix(X, gamma)\n",
        "\n",
        "    alpha = np.full(n_samples, 1 / n_samples)\n",
        "\n",
        "    # Gradient descent loop\n",
        "    for _ in range(n_iters):\n",
        "        gradient = K @ alpha\n",
        "        gradient = gradient * 2 - np.diag(K)\n",
        "\n",
        "        # Update alpha\n",
        "        alpha -= lr * gradient\n",
        "\n",
        "        # Clip values to constraint [0, 1/(nu * n)]\n",
        "        alpha = np.clip(alpha, 0, 1 / (nu * n_samples))\n",
        "\n",
        "        # Project onto simplex so sum(alpha) = 1\n",
        "        alpha = project_to_simplex(alpha)\n",
        "\n",
        "    return alpha, K"
      ],
      "metadata": {
        "id": "kvVFXrt3zqVK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Project to Simplex (Ensures sum of α = 1)\n"
      ],
      "metadata": {
        "id": "98iYEINozui0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_to_simplex(v):\n",
        "    \"\"\"Project vector v to the probability simplex (sum = 1, v_i >= 0).\"\"\"\n",
        "    n = len(v)\n",
        "    u = np.sort(v)[::-1]\n",
        "    cssv = np.cumsum(u)\n",
        "    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - 1))[0][-1]\n",
        "    theta = (cssv[rho] - 1) / (rho + 1)\n",
        "    return np.maximum(v - theta, 0)"
      ],
      "metadata": {
        "id": "02GHj11cztmH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Compute Decision Function\n",
        "\n"
      ],
      "metadata": {
        "id": "jq2kk_gQz0Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_function(x, X_train, alpha, gamma):\n",
        "    result = 0\n",
        "    for i in range(len(X_train)):\n",
        "        result += alpha[i] * rbf_kernel(x, X_train[i], gamma)\n",
        "    return result"
      ],
      "metadata": {
        "id": "RiMLySz9z1jt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Predict Labels for New Data\n"
      ],
      "metadata": {
        "id": "CTCgv3u8z4Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_test, X_train, alpha, gamma, threshold):\n",
        "    predictions = []\n",
        "    scores = []\n",
        "\n",
        "    for x in X_test:\n",
        "        score = decision_function(x, X_train, alpha, gamma)\n",
        "        label = 1 if score >= threshold else -1  # 1 = normal, -1 = anomaly\n",
        "        predictions.append(label)\n",
        "        scores.append(score)\n",
        "\n",
        "    return predictions, scores"
      ],
      "metadata": {
        "id": "ulOt3cxhz63s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Run Everything"
      ],
      "metadata": {
        "id": "7H2i7G0Fz-3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "gamma = 0.5\n",
        "nu = 0.1\n",
        "alpha, K_train = train_one_class_svm(X_train, nu=nu, gamma=gamma)\n",
        "\n",
        "# Use average of decision scores on training set as threshold\n",
        "train_scores = [decision_function(x, X_train, alpha, gamma) for x in X_train]\n",
        "threshold = np.percentile(train_scores, 100 * nu)\n",
        "\n",
        "# Predict\n",
        "predictions, scores = predict(X_test, X_train, alpha, gamma, threshold)\n",
        "\n",
        "# Display\n",
        "for i, (x, pred, score) in enumerate(zip(X_test, predictions, scores)):\n",
        "    status = \"Normal ✅\" if pred == 1 else \"Anomaly 🚨\"\n",
        "    print(f\"Test Point {i}: {x} | Score: {score:.4f} | {status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql-bsBmr0BW6",
        "outputId": "d290a35f-3b73-46e7-f914-9b8d3919ba16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Point 0: [0.35778736 0.56078453] | Score: 0.2054 | Normal ✅\n",
            "Test Point 1: [1.08305124 1.05380205] | Score: 0.2053 | Normal ✅\n",
            "Test Point 2: [-1.37766937 -0.93782504] | Score: 0.2043 | Normal ✅\n",
            "Test Point 3: [0.51503527 0.51378595] | Score: 0.2071 | Normal ✅\n",
            "Test Point 4: [0.51504769 3.85273149] | Score: 0.0619 | Anomaly 🚨\n",
            "Test Point 5: [0.57089051 1.13556564] | Score: 0.2124 | Normal ✅\n",
            "Test Point 6: [0.95400176 0.65139125] | Score: 0.2130 | Normal ✅\n",
            "Test Point 7: [-0.31526924  0.75896922] | Score: 0.2054 | Normal ✅\n",
            "Test Point 8: [-0.77282521 -0.23681861] | Score: 0.2119 | Normal ✅\n",
            "Test Point 9: [-0.48536355  0.08187414] | Score: 0.2037 | Normal ✅\n",
            "Test Point 10: [4.93119604 5.08528927] | Score: 0.0000 | Anomaly 🚨\n",
            "Test Point 11: [4.5730825  5.18166652] | Score: 0.0000 | Anomaly 🚨\n",
            "Test Point 12: [4.0610005  4.07469638] | Score: 0.0001 | Anomaly 🚨\n",
            "Test Point 13: [5.64520112 4.72038128] | Score: 0.0000 | Anomaly 🚨\n",
            "Test Point 14: [4.25412103 5.04448652] | Score: 0.0000 | Anomaly 🚨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "| Step | Action                                         |\n",
        "|:---- |:---------------------------------------------- |\n",
        "| 1    | Generate normal and test data                  |\n",
        "| 2    | Define RBF kernel                              |\n",
        "| 3    | Compute kernel matrix                          |\n",
        "| 4    | Solve dual optimization using gradient descent |\n",
        "| 5    | Project weights onto simplex (sum = 1)         |\n",
        "| 6    | Compute anomaly scores using decision function |\n",
        "| 7    | Label points with threshold                    |\n"
      ],
      "metadata": {
        "id": "InzMwFEa0Ftd"
      }
    }
  ]
}